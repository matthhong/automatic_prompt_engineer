{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Prompts with **Automatic Prompt Engineer** (APE)\n",
    "\n",
    "This notebook demonstrates how to use Automatic Prompt Engineer (APE) (arxiv link) to optimize prompts for text generation. In its simplest form, APE takes as input a dataset (a list of inputs and a list of outputs), a prompt template, and optimizes this prompt template so that it generates the outputs given the inputs.\n",
    "\n",
    "APE accomplishes this in two steps. First, it uses a language model to generate a set of candidate prompts. Then, it uses a prompt evaluation function to evaluate the quality of each candidate prompt. Finally, it returns the prompt with the highest evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a simple dataset consisting of words and their antonyms.\n",
    "words = [\"sane\", \"direct\", \"informally\", \"unpopular\", \"subtractive\", \"nonresidential\",\n",
    "    \"inexact\", \"uptown\", \"incomparable\", \"powerful\", \"gaseous\", \"evenly\", \"formality\",\n",
    "    \"deliberately\", \"off\"]\n",
    "antonyms = [\"insane\", \"indirect\", \"formally\", \"popular\", \"additive\", \"residential\",\n",
    "    \"exact\", \"downtown\", \"comparable\", \"powerless\", \"solid\", \"unevenly\", \"informality\",\n",
    "    \"accidentally\", \"on\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to define the format of the prompt that we are using.\n",
    "\n",
    "eval_template = \\\n",
    "\"\"\"Instruction: [PROMPT]\n",
    "Input: [INPUT]\n",
    "Output: [OUTPUT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Generating prompts...\n",
      "[GPT_forward] Generating 10 completions, split into 1 batches of size 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned 10 prompts. Deduplicating...\n",
      "Deduplicated to 10 prompts.\n",
      "Evaluating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
      "Retrying...\n",
      "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating prompts:   0%|          | 0/5 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/llm.py:215\u001b[0m, in \u001b[0;36mGPT_Forward.__log_probs\u001b[0;34m(self, text, log_prob_range)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    216\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig, prompt\u001b[39m=\u001b[39;49mtext)\n\u001b[1;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    138\u001b[0m (\n\u001b[1;32m    139\u001b[0m     deployment_id,\n\u001b[1;32m    140\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m     url,\n\u001b[1;32m    156\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m openai\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msk-5gZAoKytZ5AdO5q87EPHT3BlbkFJBZECLqnd3IWqL3EYBUKn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautomatic_prompt_engineer\u001b[39;00m \u001b[39mimport\u001b[39;00m ape\n\u001b[0;32m----> 9\u001b[0m result, demo_fn \u001b[39m=\u001b[39m ape\u001b[39m.\u001b[39;49msimple_ape(\n\u001b[1;32m     10\u001b[0m     eval_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     prompt_gen_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     \u001b[39m# prompt_gen_mode='insert',\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     num_prompts\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     eval_rounds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m     prompt_gen_batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m     eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     dataset\u001b[39m=\u001b[39;49m(words, antonyms),\n\u001b[1;32m     18\u001b[0m     eval_template\u001b[39m=\u001b[39;49meval_template,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/ape.py:56\u001b[0m, in \u001b[0;36msimple_ape\u001b[0;34m(dataset, eval_template, prompt_gen_template, demos_template, eval_model, prompt_gen_model, prompt_gen_mode, num_prompts, eval_rounds, prompt_gen_batch_size, eval_batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m prompt_gen_template \u001b[39m=\u001b[39m get_simple_prompt_gen_template(\n\u001b[1;32m     53\u001b[0m     prompt_gen_template, prompt_gen_mode)\n\u001b[1;32m     54\u001b[0m conf \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39msimple_config(\n\u001b[1;32m     55\u001b[0m     eval_model, prompt_gen_model, prompt_gen_mode, num_prompts, eval_rounds, prompt_gen_batch_size, eval_batch_size)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m find_prompts(eval_template, demos_template, dataset, dataset, conf, prompt_gen_template\u001b[39m=\u001b[39;49mprompt_gen_template)\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/ape.py:152\u001b[0m, in \u001b[0;36mfind_prompts\u001b[0;34m(eval_template, demos_template, prompt_gen_data, eval_data, conf, base_conf, few_shot_data, prompt_gen_template)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDeduplicated to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m prompts.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(prompts)))\n\u001b[1;32m    150\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluating prompts...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m res \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39;49mevalute_prompts(prompts, eval_template, eval_data, demos_template, few_shot_data,\n\u001b[1;32m    153\u001b[0m                                conf[\u001b[39m'\u001b[39;49m\u001b[39mevaluation\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mmethod\u001b[39;49m\u001b[39m'\u001b[39;49m], conf[\u001b[39m'\u001b[39;49m\u001b[39mevaluation\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    155\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished evaluating.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    157\u001b[0m demo_fn \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mdemo_function(eval_template, conf[\u001b[39m'\u001b[39m\u001b[39mdemo\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/evaluate.py:40\u001b[0m, in \u001b[0;36mevalute_prompts\u001b[0;34m(prompts, eval_template, eval_data, demos_template, few_shot_data, eval_method, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mReturns the scores for a list of prompts.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m    An evaluation result object.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m eval_method \u001b[39m=\u001b[39m get_eval_method(eval_method)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m eval_method(prompts, eval_template, eval_data, demos_template, few_shot_data, config)\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/evaluation/bandits.py:26\u001b[0m, in \u001b[0;36mbandits_evaluator\u001b[0;34m(prompts, eval_template, eval_data, demos_template, few_shot_data, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m sampled_prompts \u001b[39m=\u001b[39m [prompts[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m sampled_prompts_idx]\n\u001b[1;32m     25\u001b[0m \u001b[39m# Evaluate the sampled prompts\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m sampled_eval_results \u001b[39m=\u001b[39m base_eval_method(\n\u001b[1;32m     27\u001b[0m     sampled_prompts, eval_template, eval_data, demos_template, few_shot_data, config[\u001b[39m'\u001b[39;49m\u001b[39mbase_eval_config\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     28\u001b[0m _, scores \u001b[39m=\u001b[39m sampled_eval_results\u001b[39m.\u001b[39min_place(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Update the bandit algorithm\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/evaluation/likelihood.py:61\u001b[0m, in \u001b[0;36mlikelihood_evaluator\u001b[0;34m(prompts, eval_template, eval_data, demos_template, few_shot_data, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# Instantiate the LLM\u001b[39;00m\n\u001b[1;32m     59\u001b[0m model \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39mmodel_from_config(config[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 61\u001b[0m log_probs, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlog_probs(queries, output_indices)\n\u001b[1;32m     63\u001b[0m res \u001b[39m=\u001b[39m LikelihoodEvaluationResult(prompts, log_probs, config[\u001b[39m'\u001b[39m\u001b[39mnum_samples\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/llm.py:143\u001b[0m, in \u001b[0;36mGPT_Forward.log_probs\u001b[0;34m(self, text, log_prob_range)\u001b[0m\n\u001b[1;32m    140\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m text_batch, log_prob_range \u001b[39min\u001b[39;00m tqdm(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text_batches, log_prob_range_batches)),\n\u001b[1;32m    142\u001b[0m                                        disable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_tqdm):\n\u001b[0;32m--> 143\u001b[0m     log_probs_batch, tokens_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__log_probs(\n\u001b[1;32m    144\u001b[0m         text_batch, log_prob_range)\n\u001b[1;32m    145\u001b[0m     log_probs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log_probs_batch\n\u001b[1;32m    146\u001b[0m     tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tokens_batch\n",
      "File \u001b[0;32m~/Projects/APE/automatic_prompt_engineer/llm.py:220\u001b[0m, in \u001b[0;36mGPT_Forward.__log_probs\u001b[0;34m(self, text, log_prob_range)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    219\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRetrying...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m    221\u001b[0m log_probs \u001b[39m=\u001b[39m [response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][i][\u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtoken_logprobs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    222\u001b[0m              \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m]))]\n\u001b[1;32m    223\u001b[0m tokens \u001b[39m=\u001b[39m [response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][i][\u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    224\u001b[0m           \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m]))]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now, let's use APE to find prompts that generate antonyms for each word.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import openai\n",
    "openai.api_key =\"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\"\n",
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "result, demo_fn = ape.simple_ape(\n",
    "    eval_model='gpt-3.5-turbo',\n",
    "    prompt_gen_model='gpt-3.5-turbo',\n",
    "    # prompt_gen_mode='insert',\n",
    "    num_prompts=2,\n",
    "    eval_rounds=5,\n",
    "    prompt_gen_batch_size=10,\n",
    "    eval_batch_size=20,\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: prompt\n",
      "----------------\n",
      "-0.37: \n",
      "Write a program that takes a word as input and prints the antonym of the word if it exists.\n",
      "\n",
      "Here are some sample input and outputs:\n",
      "\n",
      "Input: big\n",
      "Output: small\n",
      "\n",
      "Input: tall\n",
      "Output:\n",
      "-0.55: Write a program that takes a word as input and prints out its opposite meaning.\n",
      "\n",
      "How to find the opposite meaning of a word?\n",
      "\n",
      "1. Look up a word in the dictionary.\n",
      "\n",
      "2. Write down the definition.\n",
      "\n",
      "-0.67: After you have typed the word in the input box, just click on the search button to search for its antonyms.\n",
      "\n",
      "-1.33: Write a program that changes nonresidential into residential.\n",
      "\n",
      "Example:\n",
      "\n",
      "\n",
      "Example:\n",
      "\n",
      "-1.94: After the word there is a space and then the instruction.\n",
      "\n",
      "Input: on Output: off\n",
      "\n",
      "Input: On Output: off\n",
      "\n",
      "Input: ON Output: off\n",
      "\n",
      "Input: oN Output: off\n",
      "\n",
      "-3.13: After you have listened to the audio file, choose the correct word from the drop down menu to complete each sentence.\n",
      "\n",
      "Level: Beginner\n",
      "\n",
      "-3.29: Question 18\n",
      "\n",
      "You are given the following statements:\n",
      "\n",
      "n n+1 n+2\n",
      "---- ---- ----\n",
      "on  off  off\n",
      "\n",
      "Find out what will be the value of:\n",
      "\n",
      "n+10\n",
      "\n",
      "?\n",
      "\n",
      "-3.60: After you have completed all of the activities, write a one paragraph reflection on what you have learned in this lesson.\n",
      "\n",
      "Reflection:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-3.92: After the input information, please click \"Calculate\" button.\n",
      "\n",
      "Name:\n",
      "\n",
      "Age:\n",
      "\n",
      "Sex: female male\n",
      "\n",
      "smoke: yes no\n",
      "\n",
      "drink: yes no\n",
      "\n",
      "have diabetes: yes no\n",
      "\n",
      "-4.06: Write a program that takes an input of a word and outputs the word with the opposite spelling. If the word doesn't start with a vowel, you do not have to change the spelling.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the results.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with a prompt written by a human:\n",
    "\n",
    "\"*Write an antonym to the following word.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatic_prompt_engineer import ape\n",
    "\n",
    "manual_prompt = \"Write an antonym to the following word.\"\n",
    "\n",
    "human_result = ape.simple_eval(\n",
    "    dataset=(words, antonyms),\n",
    "    eval_template=eval_template,\n",
    "    prompts=[manual_prompt],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(p): prompt\n",
      "----------------\n",
      "-0.24: Write an antonym to the following word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(human_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2afcb7a2e6fcb9490d448e607abf9226c3f7acca28baeea9bc24b456562037f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
